# while global_step < training_step:
#     # 新的轨迹  
#     episode_num += 1
#     step_in_ep = 0  # 本轨迹内用的总步数
#     total_reward = 0
#     done = False
#     obs_n = self._env.reset(1)   # envwrapper中的初始环境
#     '''从PP环境中返回的obs_n、reward、done、info都是list。
#     ic3net返回的obs是（tuple(tuple（数, 数, array（1，1，59）)）,而且三个array的形状还不一样，在envwrapper中转换成了list'''
    
#     info_n = self._env.env.get_state()   
#     '''PP 中：
#     # info_n是一个字典list，每个字典键值对为state: 行array        list中共n_agent个字典。其实只有第一个有用，因为它们都一样的
#     # state行array的内容是图中 <所有agent的方位信息>即全局信息，大小为 agent_num*2(x,y)
#     '''
    
#     h_schedule_n = np.zeros(self._n_predator)  # schedule history
#     # 类型是 np.array，包含每个agent的调度值
    
#     # 在obs和state后面分别添加历史信息，正如 第58行所说，同时将obs的数据类型修改为ndarray
#     obs_n, state, _ = self.get_obs_state_with_schedule(obs_n, info_n, h_schedule_n, init=True)
#     ''' <schednet要求的最后的obs_n的数据类型是二维array> 
#         pp环境的obs(最后一列已经加了调度值)最开始的一次观测：
#         0:array([0.66666667, 0.83333333, 1.        , 1.        , 0.        ,0.5       , 0.        ])
#         1:array([0.66666667, 1.        , 0.        , 0.        , 0.5       ,0.5       , 0.        ])
#         2:array([0.16666667, 0.33333333, 0.        , 0.        , 0.5       ,0.5       , 0.        ])
#         3:array([0.83333333, 0.33333333, 1.        , 1.        , 1.        ,0.        , 0.        ])
#         4:array([0.66666667, 0.5       ])
#     '''
    
#     while not done: # 本轨迹未结束
#         global_step += 1
#         step_in_ep += 1

#         schedule_n, priority = self.get_schedule(obs_n, global_step, FLAGS.sched)
        
#         action_n = self.get_action(obs_n, schedule_n, global_step)
        
#         obs_n_without_schedule, reward_n, done_n, info_n = self._env.step(action_n)
#         '''从PP环境中返回的obs_n、reward、done、info都是list。
#         ic3net返回的obs是（tuple(tuple（数, 数, array（1，1，59）)）,而且三个array的形状还不一样，在envwrapper中转换成了list
#         reward_n其实不必由ndarray转换为list因为train中反正要求和，done反正PP代码中也要对所有的done求和，干脆返回TF中的episode_over，一个bool类型的数'''
#         # reward_n = reward_n.tolist()
        
#         obs_n_next, state_next, h_schedule_n = self.get_obs_state_with_schedule(obs_n_without_schedule, info_n, h_schedule_n, schedule_n)
#         done_single = done_n    # bool类型即可
#         '''
#         schednet中state是ndarray，里面有14个元素（2*5+4），又忘了？是全局的方位信息！
#         reward是list，里面有5个数，是每个agent获得的奖励值
#         priority是ndarray，里面有4个数，应该是agent的调度权重？
#         done_n是list，里面是False或者True，done_single是bool类型 
#         '''
        
#         # 传入train_agents中的都是ndarray
#         self.train_agents(state, obs_n, action_n, reward_n, state_next, obs_n_next, schedule_n, priority, done_single)
        
#         if FLAGS.gui:
#             self.canvas.draw(state_next * FLAGS.map_size, [0]*self._n_predator, "Train")
        
#         # 更新！！！
#         obs_n = obs_n_next
#         state = state_next
#         total_reward += np.sum(reward_n)

#         if is_episode_done(done_n, step_in_ep):    # 应该再加个step_in_ep<500？还是说训练时每一条轨迹一口气走到黑
#             if FLAGS.gui:
#                 self.canvas.draw(state_next * FLAGS.map_size, [0]*self._n_predator, "Train", True)
#             if print_flag:
#                 print("[train_episode %d]" % (episode_num)," total step till now:", global_step, " step_used:", step_in_ep, " reward", total_reward)
#             done = True

#         if FLAGS.eval_on_train and global_step % FLAGS.eval_step == 0:
#             self.test(global_step)
#             break